{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2212bca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import numpy as np \n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from gensim.models import TfidfModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3b8a19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_json(\"FIRE_cleaned_data.json\", lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33234083",
   "metadata": {},
   "source": [
    "# Sentimental Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f79ca05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Blob\n",
    "def calculate_sentiment(tokens):\n",
    "    text = ' '.join(tokens)  # Convert tokens back to a single string\n",
    "    blob = TextBlob(text)\n",
    "    polarity = blob.sentiment.polarity\n",
    "    subjectivity = blob.sentiment.subjectivity\n",
    "    \n",
    "    if polarity > 0:\n",
    "        sentiment = \"positive\"\n",
    "    elif polarity < 0:\n",
    "        sentiment = \"negative\"\n",
    "    else:\n",
    "        sentiment = \"neutral\"\n",
    "    return polarity, subjectivity,sentiment\n",
    "\n",
    "# Apply the sentiment calculation function to each row\n",
    "df[['textblob_polarity', 'textblob_subjectivity','textblob_sentiment']] = df['lemmatized_tokens'].apply(calculate_sentiment).apply(pd.Series)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09c9b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VADER \n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Create a function to calculate sentiment scores\n",
    "def get_sentiment_scores(tokens):\n",
    "    # Combine the lemmatized tokens back into a text\n",
    "    text = ' '.join(tokens)\n",
    "    \n",
    "    # Calculate sentiment scores\n",
    "    sentiment = analyzer.polarity_scores(text)\n",
    "    compound_score = sentiment['compound']\n",
    "    \n",
    "    if compound_score >= 0.05:\n",
    "        sentiment_label = 'Positive'\n",
    "    elif compound_score <= -0.05:\n",
    "        sentiment_label = 'Negative'\n",
    "    else:\n",
    "        sentiment_label = 'Neutral'\n",
    "    \n",
    "    return compound_score, sentiment_label\n",
    "\n",
    "# Calculate sentiment scores for each row and store them in new columns\n",
    "df[['vader_compound_score', 'vader_sentiment']] = df['lemmatized_tokens'].apply(get_sentiment_scores).apply(pd.Series)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce713363",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_counts = df.groupby('textblob_sentiment').size()\n",
    "vader_sentiment_counts = df.groupby('vader_sentiment').size()\n",
    "\n",
    "# Create a single graph for sentiment and Vader sentiment vs frequency\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "# Plot sentiment counts\n",
    "plt.barh(sentiment_counts.index, sentiment_counts.values, color='grey', label='TextBlob Sentiment')\n",
    "\n",
    "# Plot Vader sentiment counts\n",
    "plt.barh(vader_sentiment_counts.index, vader_sentiment_counts.values, color='olive', label='Vader Sentiment')\n",
    "\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Sentiment')\n",
    "plt.title('TextBlob Sentiment vs Vader Sentiment Frequencies')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3896e07c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Group data by sentiment and merge tokens\n",
    "grouped = df.groupby('textblob_sentiment')['lemmatized_tokens'].apply(lambda x: [token for sublist in x for token in sublist])\n",
    "\n",
    "# Count word frequencies in each sentiment group\n",
    "word_counts = {sentiment: Counter(tokens) for sentiment, tokens in grouped.items()}\n",
    "\n",
    "# Set up subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(10, 10))\n",
    "\n",
    "# Create word clouds for each sentiment group and display in subplots\n",
    "for i, (sentiment, counts) in enumerate(word_counts.items()):\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(counts)\n",
    "    axes[i].imshow(wordcloud, interpolation='bilinear')\n",
    "    axes[i].set_title(f'{sentiment.capitalize()} Sentiment')\n",
    "    axes[i].axis('off')\n",
    "\n",
    "# Adjust layout spacing\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565d5228",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Group data by sentiment and merge tokens\n",
    "grouped = df.groupby('vader_sentiment')['lemmatized_tokens'].apply(lambda x: [token for sublist in x for token in sublist])\n",
    "\n",
    "# Count word frequencies in each sentiment group\n",
    "word_counts = {sentiment: Counter(tokens) for sentiment, tokens in grouped.items()}\n",
    "\n",
    "# Set up subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(10, 10))\n",
    "\n",
    "# Create word clouds for each sentiment group and display in subplots\n",
    "for i, (sentiment, counts) in enumerate(word_counts.items()):\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(counts)\n",
    "    axes[i].imshow(wordcloud, interpolation='bilinear')\n",
    "    axes[i].set_title(f'{sentiment.capitalize()} Sentiment')\n",
    "    axes[i].axis('off')\n",
    "\n",
    "# Adjust layout spacing\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3addcca",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "grouped = df.groupby(['date', 'vader_sentiment']).size().unstack(fill_value=0)\n",
    "\n",
    "\n",
    "grouped.plot(kind='bar', figsize=(10, 6))\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Sentiment Counts by Date')\n",
    "plt.legend(title='Sentiment')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a21c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting DataFrame by 'vader_compound_score'\n",
    "df_sorted = df.sort_values(by='vader_compound_score', ascending=False)\n",
    "\n",
    "# Retrieving the top 5 and bottom 5 comments with their corresponding titles, bodies, and scores\n",
    "top_comments = df_sorted.nlargest(5, 'vader_compound_score')[['post_title', 'comment', 'vader_compound_score']]\n",
    "bottom_comments = df_sorted.nsmallest(5, 'vader_compound_score')[['post_title', 'comment', 'vader_compound_score']]\n",
    "\n",
    "# Printing the results\n",
    "print(\"Top 5 Comments with Highest Compound Scores:\")\n",
    "print(top_comments)\n",
    "\n",
    "print(\"\\nBottom 5 Comments with Lowest Compound Scores:\")\n",
    "print(bottom_comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7758b757",
   "metadata": {},
   "source": [
    "# Topic modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c9fb19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4949b2c",
   "metadata": {},
   "source": [
    "BOW method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459dc6b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dictionary_bow = corpora.Dictionary(df['lemmatized_tokens'])\n",
    "\n",
    "# Create a corpus (bag of words representation) from the unigrams\n",
    "corpus = [dictionary_bow.doc2bow(unigram) for unigram in df['lemmatized_tokens']]\n",
    "\n",
    "# Build the LDA model\n",
    "lda_model_bow = LdaModel(corpus, num_topics= 4, id2word=dictionary_bow, passes=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313af209",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Print the topics and their top terms\n",
    "topics_bow = lda_model_bow.print_topics(num_words=50)\n",
    "for topic in topics_bow:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e906fd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initialize an empty list to store the topics\n",
    "topics__bow_list = []\n",
    "\n",
    "# Parse the topics and extract terms\n",
    "for topic_id, terms_line in topics_bow:\n",
    "    # Extract terms\n",
    "    terms = re.findall(r'\"([^\"]+)\"', terms_line)\n",
    "    \n",
    "    # Append the list of terms to the topics_list\n",
    "    topics__bow_list.append(terms)\n",
    "\n",
    "\n",
    "# Create word clouds for each topic\n",
    "for i, topic in enumerate(topics__bow_list):\n",
    "    # Convert the list of terms into a space-separated string\n",
    "    text = \" \".join(topic)\n",
    "    \n",
    "    # Create a WordCloud object\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
    "    \n",
    "    # Display the WordCloud\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.title(f\"Topic {i} Word Cloud\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcbc70a",
   "metadata": {},
   "source": [
    "TF-IDF method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0250ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_tf = corpora.Dictionary(df['lemmatized_tokens'])\n",
    "\n",
    "# Create a corpus (bag of words representation) from the unigrams\n",
    "corpus_bow = [dictionary_tf.doc2bow(unigram) for unigram in df['lemmatized_tokens']]\n",
    "\n",
    "# Build the TF-IDF model\n",
    "tfidf_model = TfidfModel(corpus_bow)\n",
    "\n",
    "# Apply TF-IDF transformation to the bag of words corpus\n",
    "corpus_tfidf = [tfidf_model[doc] for doc in corpus_bow]\n",
    "\n",
    "# Build the LDA model using TF-IDF corpus\n",
    "lda_model_tf = LdaModel(corpus_tfidf, num_topics=4, id2word=dictionary_tf, passes=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd27af39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the topics and their top terms\n",
    "topics_tf = lda_model_tf.print_topics(num_words=50)\n",
    "for topic in topics_tf:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b9ae21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store the topics\n",
    "topics_tf_list = []\n",
    "\n",
    "# Parse the topics and extract terms\n",
    "for topic_id, terms_line in topics_tf:\n",
    "    # Extract terms\n",
    "    terms = re.findall(r'\"([^\"]+)\"', terms_line)\n",
    "    \n",
    "    # Append the list of terms to the topics_list\n",
    "    topics_tf_list.append(terms)\n",
    "\n",
    "\n",
    "# Create word clouds for each topic\n",
    "for i, topic in enumerate(topics_tf_list):\n",
    "    # Convert the list of terms into a space-separated string\n",
    "    text = \" \".join(topic)\n",
    "    \n",
    "    # Create a WordCloud object\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
    "    \n",
    "    # Display the WordCloud\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.title(f\"Topic {i} Word Cloud\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e076c46b",
   "metadata": {},
   "source": [
    "Analysis of LDA model which was trained on BOW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b725cf9",
   "metadata": {},
   "source": [
    "Topic-0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e947bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows based on the presence of specific words in lemmatized_tokens\n",
    "desired_words = [\"comment\", \"please\", \"need\", \"contact\", \"moderators\"]\n",
    "filtered_rows = df[df['lemmatized_tokens'].apply(lambda tokens: any(word in tokens for word in desired_words))]\n",
    "\n",
    "# Create a new DataFrame with the filtered rows and selected columns\n",
    "topic_0_df = filtered_rows[['post_title', 'comment', 'vader_compound_score', 'vader_sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558d2f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting DataFrame by 'vader_compound_score'\n",
    "df_sorted = topic_0_df.sort_values(by='vader_compound_score', ascending=False)\n",
    "\n",
    "# Retrieving the top 5 and bottom 5 comments with their corresponding titles, bodies, and scores\n",
    "top_comments = df_sorted.nlargest(5, 'vader_compound_score')[['post_title', 'comment', 'vader_compound_score', 'vader_sentiment']]\n",
    "bottom_comments = df_sorted.nsmallest(5, 'vader_compound_score')[['post_title', 'comment', 'vader_compound_score', 'vader_sentiment']]\n",
    "\n",
    "# Printing the results\n",
    "print(\"Top 5 Comments with Highest Compound Scores:\")\n",
    "print(top_comments)\n",
    "\n",
    "print(\"\\nBottom 5 Comments with Lowest Compound Scores:\")\n",
    "print(bottom_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1097dd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a countplot to visualize sentiment distribution\n",
    "plt.figure(figsize=(5, 5))\n",
    "sns.set(style=\"whitegrid\")\n",
    "sns.countplot(x='vader_sentiment', data=topic_0_df)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Sentiments in Comments')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7f10b3",
   "metadata": {},
   "source": [
    "Topic-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bffd4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows based on the presence of specific words in lemmatized_tokens\n",
    "desired_words = [\"car\", \"look\", \"drive\", \"love\", \"great\"]\n",
    "filtered_rows = df[df['lemmatized_tokens'].apply(lambda tokens: any(word in tokens for word in desired_words))]\n",
    "\n",
    "# Create a new DataFrame with the filtered rows and selected columns\n",
    "topic_0_df = filtered_rows[['post_title', 'comment', 'vader_compound_score', 'vader_sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12288792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting DataFrame by 'vader_compound_score'\n",
    "df_sorted = topic_0_df.sort_values(by='vader_compound_score', ascending=False)\n",
    "\n",
    "# Retrieving the top 5 and bottom 5 comments with their corresponding titles, bodies, and scores\n",
    "top_comments = df_sorted.nlargest(5, 'vader_compound_score')[['post_title', 'comment', 'vader_compound_score', 'vader_sentiment']]\n",
    "bottom_comments = df_sorted.nsmallest(5, 'vader_compound_score')[['post_title', 'comment', 'vader_compound_score', 'vader_sentiment']]\n",
    "\n",
    "# Printing the results\n",
    "print(\"Top 5 Comments with Highest Compound Scores:\")\n",
    "print(top_comments)\n",
    "\n",
    "print(\"\\nBottom 5 Comments with Lowest Compound Scores:\")\n",
    "print(bottom_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6cca19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a countplot to visualize sentiment distribution\n",
    "plt.figure(figsize=(5, 5))\n",
    "sns.set(style=\"whitegrid\")\n",
    "sns.countplot(x='vader_sentiment', data=topic_0_df)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Sentiments in Comments')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af53c00",
   "metadata": {},
   "source": [
    "Topic-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d926a1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows based on the presence of specific words in lemmatized_tokens\n",
    "desired_words = [\"tire\", \"work\", \"part\", \"issue\", \"engine\"]\n",
    "filtered_rows = df[df['lemmatized_tokens'].apply(lambda tokens: any(word in tokens for word in desired_words))]\n",
    "\n",
    "# Create a new DataFrame with the filtered rows and selected columns\n",
    "topic_0_df = filtered_rows[['post_title', 'comment', 'vader_compound_score', 'vader_sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e307330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting DataFrame by 'vader_compound_score'\n",
    "df_sorted = topic_0_df.sort_values(by='vader_compound_score', ascending=False)\n",
    "\n",
    "# Retrieving the top 5 and bottom 5 comments with their corresponding titles, bodies, and scores\n",
    "top_comments = df_sorted.nlargest(5, 'vader_compound_score')[['post_title', 'comment', 'vader_compound_score', 'vader_sentiment']]\n",
    "bottom_comments = df_sorted.nsmallest(5, 'vader_compound_score')[['post_title', 'comment', 'vader_compound_score', 'vader_sentiment']]\n",
    "\n",
    "# Printing the results\n",
    "print(\"Top 5 Comments with Highest Compound Scores:\")\n",
    "print(top_comments)\n",
    "\n",
    "print(\"\\nBottom 5 Comments with Lowest Compound Scores:\")\n",
    "print(bottom_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839662b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a countplot to visualize sentiment distribution\n",
    "plt.figure(figsize=(5, 5))\n",
    "sns.set(style=\"whitegrid\")\n",
    "sns.countplot(x='vader_sentiment', data=topic_0_df)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Sentiments in Comments')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8476d51c",
   "metadata": {},
   "source": [
    "Topic-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cc64c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows based on the presence of specific words in lemmatized_tokens\n",
    "desired_words = [\"better\", \"sell\", \"buy\", \"old\", \"money\"]\n",
    "filtered_rows = df[df['lemmatized_tokens'].apply(lambda tokens: any(word in tokens for word in desired_words))]\n",
    "\n",
    "# Create a new DataFrame with the filtered rows and selected columns\n",
    "topic_0_df = filtered_rows[['post_title', 'comment', 'vader_compound_score', 'vader_sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c2a545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting DataFrame by 'vader_compound_score'\n",
    "df_sorted = topic_0_df.sort_values(by='vader_compound_score', ascending=False)\n",
    "\n",
    "# Retrieving the top 5 and bottom 5 comments with their corresponding titles, bodies, and scores\n",
    "top_comments = df_sorted.nlargest(5, 'vader_compound_score')[['post_title', 'comment', 'vader_compound_score', 'vader_sentiment']]\n",
    "bottom_comments = df_sorted.nsmallest(5, 'vader_compound_score')[['post_title', 'comment', 'vader_compound_score', 'vader_sentiment']]\n",
    "\n",
    "# Printing the results\n",
    "print(\"Top 5 Comments with Highest Compound Scores:\")\n",
    "print(top_comments)\n",
    "\n",
    "print(\"\\nBottom 5 Comments with Lowest Compound Scores:\")\n",
    "print(bottom_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33537db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a countplot to visualize sentiment distribution\n",
    "plt.figure(figsize=(5, 5))\n",
    "sns.set(style=\"whitegrid\")\n",
    "sns.countplot(x='vader_sentiment', data=topic_0_df)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Sentiments in Comments')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29a4115",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
